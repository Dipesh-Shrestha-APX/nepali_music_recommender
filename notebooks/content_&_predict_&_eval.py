# -*- coding: utf-8 -*-
"""Content & Predict & Eval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xY9QuAcjqo60ZbaLDfjbYWB_o9lIXZ0F
"""

#Import all the required packages
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np
import random
import pickle
import os

# Load the songs.csv file into a Pandas dataframe
songs = pd.read_csv('/content/DATASET FOR TRAINING.csv')
# songs = songs.head(10)

# print(songs.iloc[341])
# print(songs.iloc[241])

# Combine the columns for each song into a single string
# for each row of df and store it in df

def combine_features(row):
    artist = str(row['artist']).replace(' ', '')
    # print(artist)
    artist = artist.replace(',', ' ')
    # print(artist)
    genre = str(row['genre']).replace('|', ' ')
    tempo = str(row['tempo'])
    mood = str(row['mood_tags']).replace(',', ' ')

    artist = artist.lower()
    genre = genre.lower()
    mood = mood.lower()

    return f"{artist} {genre} {tempo} {mood}"

songs['combined_features'] = songs.apply(combine_features, axis=1)
songs['combined_features']
songs.head(5)

"""# CONTENT BASED RECOMMENDATION"""

# Create a TfidfVectorizer object to transform the song genre into a Tf-idf representation
tfidf = TfidfVectorizer()
tfidf_matrix = tfidf.fit_transform(songs['combined_features'])
# tfidf_matrix[0].toarray().flatten().shape
tfidf_matrix.shape
# total (Rows, BOW)

# Calculate the cosine similarity matrix between the songs
cosine_similarity = cosine_similarity(tfidf_matrix)
cosine_similarity.shape

# Create a dataframe with the cosine similarity scores
similarity_df = pd.DataFrame(cosine_similarity, index=songs['title'], columns=songs['title'])
similarity_df.shape
similarity_df.head(5)

# Step 6: Save Model Components
# -----------------------------
os.makedirs("/content/drive/MyDrive/content_based/model/", exist_ok=True)

with open("/content/drive/MyDrive/content_based/model/tfidf_vectorizer.pkl", "wb") as f:
    pickle.dump(tfidf, f)

with open("/content/drive/MyDrive/content_based/model/tfidf_matrix.pkl", "wb") as f:
    pickle.dump(tfidf_matrix, f)

with open("/content/drive/MyDrive/content_based/model/similarity_matrix.pkl", "wb") as f:
    pickle.dump(similarity_df, f)

songs.to_csv("/content/drive/MyDrive/content_based/model/songs_with_features.csv", index=False)

print("Model and data saved in '/content/drive/MyDrive/model/content_based/' directory.")

#How to load pickle model and use it later on

# Load saved files
with open("/content/drive/MyDrive/content_based/model/tfidf_vectorizer.pkl", "rb") as f:
    tfidf = pickle.load(f)

with open("/content/drive/MyDrive/content_based/model/tfidf_matrix.pkl", "rb") as f:
    tfidf_matrix = pickle.load(f)

with open("/content/drive/MyDrive/content_based/model/similarity_matrix.pkl", "rb") as f:
    similarity_df = pickle.load(f)

songs = pd.read_csv("/content/drive/MyDrive/content_based/model/songs_with_features.csv")

def recommend(song_reference, similarity_df):
    # Find the index of the user's Song in the similarity dataframe
    song_index = similarity_df.index.get_loc(song_reference)

    # Get the top 10 most similar songs to the user's Song
    top_10 = similarity_df.iloc[song_index].sort_values(ascending=False)[1:11]

    # Print the top 10 most similar songs to the user's Song
    print(f'Top 10 similar songs to {song_reference}:')
    print(top_10)
# Step 3: Run a test
# -----------------------------
user_input = input("\nEnter a song title you like: ")
recommend(user_input, similarity_df)

# -*- coding: utf-8 -*-
"""RecommendationSystem_Evaluation.ipynb

This notebook evaluates the recommendation system using Precision@10 and MAP.
It loads model components and user_liked_songs.csv from Google Drive for evaluation.
"""

# Import required packages
import pandas as pd
import numpy as np
import random
import pickle
import os
from sklearn.metrics import average_precision_score
from google.colab import drive

# Step 1: Mount Google Drive
# --------------------------
# drive.mount('/content/drive')

# Step 2: Load Saved Model Components
# -----------------------------------
model_dir = "/content/drive/MyDrive/content_based/model/"  # Adjust this path to your folder
try:
    with open(os.path.join(model_dir, "tfidf_vectorizer.pkl"), "rb") as f:
        tfidf = pickle.load(f)

    with open(os.path.join(model_dir, "tfidf_matrix.pkl"), "rb") as f:
        tfidf_matrix = pickle.load(f)

    with open(os.path.join(model_dir, "similarity_matrix.pkl"), "rb") as f:
        similarity_df = pickle.load(f)

    songs = pd.read_csv(os.path.join(model_dir, "songs_with_features.csv"))

    print("Loaded model components and songs data from Google Drive.")
except FileNotFoundError as e:
    print(f"Error: Could not find model files in '{model_dir}'. Please check the path.")
    raise e

# Step 3: Handle Duplicates in songs and similarity_df
# ---------------------------------------------------
# Deduplicate songs
if songs["title"].duplicated().any():
    print(f"Found {songs['title'].duplicated().sum()} duplicate song titles in songs")
    songs = songs.drop_duplicates(subset="title", keep="first")
    print(f"New songs shape: {songs.shape}")

# Deduplicate similarity_df
if similarity_df.index.duplicated().any():
    print(f"Found {similarity_df.index.duplicated().sum()} duplicate indices in similarity_df")
    similarity_df = similarity_df.loc[~similarity_df.index.duplicated(keep="first")]
    print(f"New similarity_df shape: {similarity_df.shape}")

# Ensure similarity_df is square
if not similarity_df.index.equals(similarity_df.columns):
    print("Warning: similarity_df is not a square matrix. Aligning index and columns...")
    valid_titles = songs["title"].values
    similarity_df = similarity_df.loc[valid_titles, valid_titles]
    print(f"Aligned similarity_df shape: {similarity_df.shape}")

# Step 4: Load User Liked Songs
# -----------------------------
user_data_path = "/content/drive/MyDrive/content_based/user_liked_songs.csv"
try:
    df_likes = pd.read_csv(user_data_path)
    print(f"Loaded user_liked_songs.csv from '{user_data_path}'")
except FileNotFoundError as e:
    print(f"Error: Could not find user_liked_songs.csv at '{user_data_path}'")
    raise e

# Convert to dictionary: {user_id: set(liked_song_titles)}
user_liked_songs = df_likes.groupby("user_id")["liked_song_title"].apply(set).to_dict()
print(f"Loaded liked songs for {len(user_liked_songs)} users")

# Step 5: Evaluate Recommendations
# -------------------------------
def evaluate_recommendations(similarity_df, user_liked_songs, k=10):
    precision_scores = []
    ap_scores = []

    for user, liked_songs in user_liked_songs.items():
        if not liked_songs:
            continue

        # Use one liked song as input for recommendation
        input_song = random.choice(list(liked_songs))

        try:
            if input_song not in similarity_df.index:
                raise KeyError(f"Song '{input_song}' not found in similarity matrix")

            # Get similarity scores
            similarity_scores = similarity_df.loc[input_song]
            if isinstance(similarity_scores, pd.DataFrame):
                if similarity_scores.shape[0] == 1:
                    similarity_scores = similarity_scores.iloc[0]  # Convert single-row DataFrame to Series
                else:
                    raise ValueError(f"Expected a Series or single-row DataFrame for {input_song}, got DataFrame with {similarity_scores.shape[0]} rows and {similarity_scores.shape[1]} columns")

            # Get top k recommendations (excluding the input song)
            top_k = similarity_scores.sort_values(ascending=False)[1:k+1]
            recommended_songs = top_k.index.tolist()

            # Ground truth: other liked songs by the user
            relevant_songs = liked_songs - {input_song}

            # Precision@k
            if relevant_songs:
                relevant_count = len(set(recommended_songs) & relevant_songs)
                precision = relevant_count / k
                precision_scores.append(precision)
            else:
                precision_scores.append(0.0)

            # MAP: Average Precision
            y_true = [1 if song in relevant_songs else 0 for song in recommended_songs]
            if any(y_true):
                y_scores = list(range(k, 0, -1))  # Assign decreasing scores from k to 1
                ap = average_precision_score(y_true, y_scores)
                ap_scores.append(ap)
            else:
                ap_scores.append(0.0)

        except KeyError as e:
            print(e)
            precision_scores.append(0.0)
            ap_scores.append(0.0)
        except Exception as e:
            print(f"Error evaluating user {user} with input song {input_song}: {e}")
            precision_scores.append(0.0)
            ap_scores.append(0.0)

    # Compute average metrics
    avg_precision = np.mean(precision_scores) if precision_scores else 0.0
    map_score = np.mean(ap_scores) if ap_scores else 0.0

    print(f"\nEvaluation Results:")
    print(f"Average Precision@{k}: {avg_precision:.4f}")
    print(f"Mean Average Precision (MAP): {map_score:.4f}")

# Step 6: Run Evaluation
# ----------------------
print(f"Number of duplicate indices in similarity_df: {similarity_df.index.duplicated().sum()}")
evaluate_recommendations(similarity_df, user_liked_songs, k=10)

